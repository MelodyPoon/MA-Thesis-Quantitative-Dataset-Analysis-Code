# -*- coding: utf-8 -*-
"""Thesis Notebook

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-MdWMcGuXPR7TMiVzy_kta3dVPWECm5M
"""

import pandas as pd
import matplotlib.pyplot as plt
import numpy as np

from google.colab import drive
drive.mount('/content/drive')

df = pd.read_csv('/content/drive/MyDrive/ERP Coding/【interest decoded】all_nonunique_visitor_pathinteractions_statevalues.csv', encoding='latin-1', on_bad_lines='skip')

from matplotlib.colors import LinearSegmentedColormap
custom_cmap = LinearSegmentedColormap.from_list("my_map", ['#e41a1c', '#377eb8', '#4daf4a'])

df

#Preprocess 'state' column --> state_type + state_value
import ast
df_state = df.copy()

def parse_state(state_str):
    try:
        return ast.literal_eval(state_str)
    except:
        return None

df_state['parsed_state'] = df_state['state'].apply(parse_state)
df_state['parsed_state']

#Extract 'state_type' column
df_state['state_type'] = df_state['parsed_state'].apply(lambda x: x[0] if isinstance(x, tuple) and len(x) > 0 else None)
df_state['state_value'] = df_state['parsed_state'].apply(lambda x: x[1] if isinstance(x, tuple) and len(x) > 1 else None)
df_state = df_state.drop(columns=['parsed_state', 'state'])
df_state

#Find out which state_type is the valid result of each installation
state_type_counts = df_state.groupby(['installation', 'state_type'])['state_type'].count().reset_index(name='count')
top_state_types_per_installation = state_type_counts.sort_values(['installation', 'count'], ascending=[True, False]).groupby('installation').head(2)
top_state_types_per_installation
#based on the amount of occurrence in each installation, the one that amount the most (except for 'visitorProfileImageTransformation) is regarded for the valid result of this specific installation.

"""# Responsive Interaction

"""

# Filter the dataframe based on the responsive interaction conditions
responsive_interaction_df = df_state[
    ((df_state['installation'] == 'Breng het nieuws') & (df_state['state_type'] == 'brengHetNieuwsVideo')) |
    ((df_state['installation'] == 'Burgerjournalistiek') & (df_state['state_type'] == 'burgerjournalistiekData')) |
    ((df_state['installation'] == 'Codetaal') & (df_state['state_type'] == 'codetaalScores')) |
    ((df_state['installation'] == 'Delen verboden') & (df_state['state_type'] == 'DelenVerbodenResult')) |
    ((df_state['installation'] == 'Grenzen') & (df_state['state_type'] == 'grenzenEarnedCharacter')) |
    ((df_state['installation'] == 'Influencer') & (df_state['state_type'] == 'InfluencerResult')) |
    ((df_state['installation'] == 'Maak Vrienden') & (df_state['state_type'] == 'maakVriendenResult')) |
    ((df_state['installation'] == 'Maak het nieuws') & (df_state['state_type'] == 'maakHetNieuwsImage')) |
    ((df_state['installation'] == 'Maak het shownieuws') & (df_state['state_type'] == 'maakHetShowNieuwsImage')) |
    ((df_state['installation'] == 'Maak je game') & (df_state['state_type'] == 'maakJeGameResult')) |
    ((df_state['installation'] == 'Ontmoet de held') & (df_state['state_type'] == 'ontmoetDeHeldVideo')) |
    ((df_state['installation'] == 'Race door de tijd') & (df_state['state_type'] == 'time_score')) |
    ((df_state['installation'] == 'Studio dialoog') & (df_state['state_type'] == 'studioVideoVirtualSet')) |
    ((df_state['installation'] == 'Toon jezelf') & (df_state['state_type'] == 'ToonJezelfResult')) |
    ((df_state['installation'] == 'Verborgen verleiders') & (df_state['state_type'] == 'verborgenVerleidersScores')) |
    ((df_state['installation'] == 'studio achtervolging') & (df_state['state_type'] == 'studioVideoVirtualSet')) |
    ((df_state['installation'] == 'verleidingstechnieken overtuigen') & (df_state['state_type'] == 'verleidingstechniekenOvertuigenData')) |
    ((df_state['installation'] == 'verleidingstechnieken propaganda') & (df_state['state_type'] == 'verleidingstechniekenPropagandaData')) |
    ((df_state['installation'] == 'verleidingstechnieken reclame') & (df_state['state_type'] == 'answer_score'))
]

# Display the filtered dataframe
responsive_interaction_df

#total interaction times for each installations
responsive_interaction_times_per_installation = responsive_interaction_df.groupby('installation').size().reset_index(name='total times')
responsive_interaction_times_per_installation

#total unique visitors per installation
responsive_interaction_unique_visitors_per_installation = responsive_interaction_df.groupby('installation')['visitor_id'].nunique().reset_index(name='unique_visitor_count')
responsive_interaction_unique_visitors_per_installation

responsive_interaction_calculation = pd.merge(
    responsive_interaction_times_per_installation,
    responsive_interaction_unique_visitors_per_installation,
    on='installation'
)
responsive_interaction_calculation['average_times'] = responsive_interaction_calculation['total times'] / responsive_interaction_calculation['unique_visitor_count']
responsive_interaction_calculation

fig, ax1 = plt.subplots(figsize=(14, 8))

x = range(len(responsive_interaction_calculation))
width = 0.35

ax1.bar([i - width/2 for i in x], responsive_interaction_calculation['total times'], width=width, label='Total Interation Times', color='skyblue')
ax1.bar([i + width/2 for i in x], responsive_interaction_calculation['unique_visitor_count'], width=width, label='Unique Visitors Number', color='orange')

#axis labels
ax1.set_ylabel('Count')
ax1.set_title('Distribution of Responsive Interactions')
ax1.set_xticks(x)
ax1.set_xticklabels(responsive_interaction_calculation['installation'], rotation=45, ha='right')
ax1.legend(loc='upper center')

# scatter ploot of average times per unique visitor
ax2 = ax1.twinx()
ax2.plot(x, responsive_interaction_calculation['average_times'], 'o-', color='green', label='Average Interaction Times per Visitor')
ax2.set_ylabel('Average Interaction Times per Visitor')
ax2.legend(loc='upper right')

plt.tight_layout()
plt.show()

responsive_interaction_times_per_visitor = responsive_interaction_df.groupby('visitor_id').size().reset_index(name='interaction times')
responsive_interaction_times_per_visitor

responsive_interaction_installation_per_visitor = responsive_interaction_df.groupby('visitor_id')['installation'].nunique().reset_index(name='installations number')
responsive_interaction_installation_per_visitor

responsive_interaction_visitor_pattern = pd.merge(
    responsive_interaction_times_per_visitor,
    responsive_interaction_installation_per_visitor,
    on='visitor_id'
)
responsive_interaction_visitor_pattern

# Create the scatter plot
plt.figure(figsize=(10, 6))
plt.scatter(responsive_interaction_visitor_pattern['interaction times'], responsive_interaction_visitor_pattern['installations number'])

# Add labels and title
plt.xlabel('Number of Responsive Interactions')
plt.ylabel('Number of Unique Installations Interacted With')
plt.title('Responsive Interaction Pattern by Visitors')
plt.grid(True)

# Show the plot
plt.show()

from sklearn.cluster import KMeans
import matplotlib.pyplot as plt

features_responsive = responsive_interaction_visitor_pattern[['interaction times', 'installations number']]
wcss = []
for i in range(1, 11):
    kmeans = KMeans(n_clusters=i, random_state=42)
    kmeans.fit(features_responsive)
    wcss.append(kmeans.inertia_)

plt.plot(range(1, 11), wcss)
plt.title('Elbow Method')
plt.xlabel('Number of clusters')
plt.ylabel('WCSS')
plt.show()

import matplotlib.pyplot as plt
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler

# Select the features for clustering
features_responsive = responsive_interaction_visitor_pattern[['interaction times', 'installations number']]

# Standardize the data
scaler = StandardScaler()
features_scaled = scaler.fit_transform(features_responsive)


kmeans = KMeans(n_clusters=3, random_state=42, n_init=10) # Add n_init
responsive_interaction_visitor_pattern['cluster'] = kmeans.fit_predict(features_responsive)

# Visualize the clusters
plt.figure(figsize=(10, 6))
plt.scatter(responsive_interaction_visitor_pattern['interaction times'], responsive_interaction_visitor_pattern['installations number'], c=responsive_interaction_visitor_pattern['cluster'], cmap=custom_cmap)
plt.xlabel('Number of Responsive Interactions')
plt.ylabel('Number of Unique Installations Interacted With')
plt.title('K-Means Clustering of Visitor Interaction Patterns')
plt.colorbar(label='Cluster')
plt.grid(True)
plt.show()

# Display the number of visitors in each cluster
print("Number of visitors in each cluster:")
print(responsive_interaction_visitor_pattern['cluster'].value_counts())

# Display the characteristics of each cluster (e.g., center values of the features)
cluster_centers = kmeans.cluster_centers_
cluster_features_df = pd.DataFrame(cluster_centers, columns=['interaction times', 'installations number'])
cluster_features_df['cluster'] = cluster_features_df.index
print("\nCluster Centers:")
print(cluster_features_df)

import matplotlib.pyplot as plt
from sklearn.cluster import DBSCAN

# Select the features for clustering
features_responsive = responsive_interaction_visitor_pattern[['interaction times', 'installations number']]

# Standardize the data (important for distance-based algorithms like DBSCAN)
scaler = StandardScaler()
features_scaled = scaler.fit_transform(features_responsive)

# Initialize DBSCAN
# eps: The maximum distance between two samples for one to be considered as in the neighborhood of the other.
# min_samples: The number of samples (or total weight) in a neighborhood for a point to be considered as a core point. This includes the point itself.
# You will likely need to tune these parameters based on your data
dbscan = DBSCAN(eps=0.5, min_samples=5)

# Fit the model and predict labels
# -1 indicates noise points (outliers)
labels = dbscan.fit_predict(features_scaled)

# Add the DBSCAN labels to the DataFrame
responsive_interaction_visitor_pattern['dbscan_label'] = labels

# Filter out the noise points (outliers)
responsive_interaction_visitor_pattern_cleaned_dbscan = responsive_interaction_visitor_pattern[
    responsive_interaction_visitor_pattern['dbscan_label'] != -1
]
# Display the number of outliers removed by DBSCAN
outlier_count_dbscan = (labels == -1).sum()
total_count_dbscan = len(labels)
outlier_percentage_dbscan = (outlier_count_dbscan / total_count_dbscan) * 100 if total_count_dbscan > 0 else 0

print(f"Using DBSCAN to remove Outliers number: {outlier_count_dbscan}")
print(f"Removed Outliers Percentage: {outlier_percentage_dbscan:.2f}%")

# Visualize the clusters found by DBSCAN (excluding outliers for better visualization)
plt.figure(figsize=(10, 6))
# Plot non-outliers with their cluster labels
plt.scatter(responsive_interaction_visitor_pattern_cleaned_dbscan['interaction times'],
            responsive_interaction_visitor_pattern_cleaned_dbscan['installations number'],
            cmap='skyblue',
            label='Non-Outliers')

# Optionally, plot outliers in a different color/marker
outliers_dbscan = responsive_interaction_visitor_pattern[responsive_interaction_visitor_pattern['dbscan_label'] == -1]
plt.scatter(outliers_dbscan['interaction times'],
            outliers_dbscan['installations number'],
            c='red',
            marker='x',
            label='Outliers (-1)')

plt.xlabel('Number of Responsive Interactions')
plt.ylabel('Number of Unique Installations Interacted With')
plt.title('DBSCAN Clustering of Visitor Interaction Patterns (Outliers in Red)')
plt.legend()
plt.grid(True)
plt.show()

# Display the number of visitors in each cluster found by DBSCAN
print("\nNumber of visitors in each cluster (DBSCAN):")
print(responsive_interaction_visitor_pattern['dbscan_label'].value_counts())

from sklearn.cluster import KMeans
import matplotlib.pyplot as plt

features_responsive_cleaned = responsive_interaction_visitor_pattern_cleaned_dbscan[['interaction times', 'installations number']]
wcss = []
for i in range(1, 11):
    kmeans = KMeans(n_clusters=i, random_state=42)
    kmeans.fit(features_responsive_cleaned)
    wcss.append(kmeans.inertia_)

plt.plot(range(1, 11), wcss)
plt.title('Elbow Method (Cleaned)')
plt.xlabel('Number of clusters')
plt.ylabel('WCSS')
plt.show()

import matplotlib.pyplot as plt
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler

# Select the features for clustering
features_responsive_cleaned = responsive_interaction_visitor_pattern_cleaned_dbscan[['interaction times', 'installations number']]

# Standardize the data
scaler = StandardScaler()
features_scaled = scaler.fit_transform(features_responsive_cleaned)


kmeans = KMeans(n_clusters=3, random_state=42, n_init=10) # Add n_init
responsive_interaction_visitor_pattern_cleaned_dbscan['cluster'] = kmeans.fit_predict(features_responsive_cleaned)

# Visualize the clusters
plt.figure(figsize=(10, 6))
plt.scatter(responsive_interaction_visitor_pattern_cleaned_dbscan['interaction times'], responsive_interaction_visitor_pattern_cleaned_dbscan['installations number'], c=responsive_interaction_visitor_pattern_cleaned_dbscan['cluster'], cmap= custom_cmap)
plt.xlabel('Number of Responsive Interactions')
plt.ylabel('Number of Unique Installations Interacted With')
plt.title('K-Means Clustering of Visitor Responsive Interaction Patterns')
plt.colorbar(label='Cluster')
plt.grid(True)
plt.show()

# Display the number of visitors in each cluster
print("Number of visitors in each cluster:")
print(responsive_interaction_visitor_pattern_cleaned_dbscan['cluster'].value_counts())

# Display the characteristics of each cluster (e.g., center values of the features)
cluster_centers = kmeans.cluster_centers_
cluster_features_df = pd.DataFrame(cluster_centers, columns=['interaction times', 'installations number'])
cluster_features_df['cluster'] = cluster_features_df.index
print("\nCluster Centers:")
print(cluster_features_df)

interaction_installation_combination_counts = responsive_interaction_visitor_pattern_cleaned_dbscan.groupby(['interaction times', 'installations number', 'cluster']).size().reset_index(name='unique visitor count')
interaction_installation_combination_counts

plt.figure(figsize=(10, 6)) # Increase figure size for better visibility if needed

# Use the aggregated data for plotting
plt.scatter(
    interaction_installation_combination_counts['interaction times'],
    interaction_installation_combination_counts['installations number'],
    s=interaction_installation_combination_counts['unique visitor count'] * 5, # 's' is the size parameter. Multiply by a factor to make sizes visible.
    alpha=0.65, # Add some transparency
    edgecolors='w', # White edge color for better visibility
    linewidth=0.5,
    c=interaction_installation_combination_counts['cluster'],
    cmap= custom_cmap
)

# Add labels and title
plt.xlabel('Number of Responsive Interactions')
plt.ylabel('Number of Unique Installations Interacted With')
plt.title('Responsive Interaction Pattern by Visitor (Size by Count)')
plt.grid(True)

plt.colorbar(label='Cluster')
# Optional: Add a legend for size
# This is a bit trickier with scatter plots and custom sizes.
# A common way is to plot dummy points for representative sizes.
for count in [1, 5, 10, 20, 50]: # Example counts to show in legend
     plt.scatter([], [], s=count * 5, label=f'{count} visitors', color='c', alpha=0.6)
plt.legend(scatterpoints=1, frameon=False, labelspacing=1, title='Visitor Count')


# Show the plot
plt.show()

"""# Experiential Engagement"""

# Filter the dataframe based on the experiential engagement conditions
experiential_engagement_df = df_state[
    ((df_state['installation'] == 'De wereld binnen handbereik') & (df_state['state_type'] == 'visitorProfileImageTransformation')) |
    ((df_state['installation'] == 'Deel je leven') & (df_state['state_type'] == 'opinion')) |
    ((df_state['installation'] == 'Draai aan je nieuws') & (df_state['state_type'] == 'visitorProfileImageTransformation')) |
    ((df_state['installation'] == 'Er was eens') & (df_state['state_type'] == 'visitorProfileImageTransformation')) |
    ((df_state['installation'] == 'Filterbubbel') & (df_state['state_type'] == 'visitorProfileImageTransformation')) |
    ((df_state['installation'] == 'Game art') & (df_state['state_type'] == 'opinion')) |
    ((df_state['installation'] == 'Game design') & (df_state['state_type'] == 'visitorProfileImageTransformation')) |
    ((df_state['installation'] == 'Game on') & (df_state['state_type'] == 'visitorProfileImageTransformation')) |
    ((df_state['installation'] == 'Maak het sportnieuws') & (df_state['state_type'] == 'visitorProfileImageTransformation')) |
    ((df_state['installation'] == 'Mediareactor') & (df_state['state_type'] == 'opinion')) |
    ((df_state['installation'] == 'Mijlpalen Delen') & (df_state['state_type'] == 'opinion')) |
    ((df_state['installation'] == 'Mijlpalen Informeren') & (df_state['state_type'] == 'opinion')) |
    ((df_state['installation'] == 'Mijlpalen Spelen') & (df_state['state_type'] == 'opinion')) |
    ((df_state['installation'] == 'Mijlpalen verkopen') & (df_state['state_type'] == 'visitorProfileImageTransformation')) |
    ((df_state['installation'] == 'Nieuwspresentatie') & (df_state['state_type'] == 'opinion')) |
    ((df_state['installation'] == 'Nieuws of nonsens') & (df_state['state_type'] == 'visitorProfileImageTransformation')) |
    ((df_state['installation'] == 'Radionieuws') & (df_state['state_type'] == 'opinion')) |
    ((df_state['installation'] == 'Radioreclames') & (df_state['state_type'] == 'opinion')) |
    ((df_state['installation'] == 'Reaguurders') & (df_state['state_type'] == 'visitorProfileImageTransformation')) |
    ((df_state['installation'] == 'Rolmodellen en stereotypen') & (df_state['state_type'] == 'visitorProfileImageTransformation')) |
    ((df_state['installation'] == 'Series kijken') & (df_state['state_type'] == 'visitorProfileImageTransformation')) |
    ((df_state['installation'] == 'Serieus grappig') & (df_state['state_type'] == 'visitorProfileImageTransformation')) |
    ((df_state['installation'] == 'Spelshow') & (df_state['state_type'] == 'opinion')) |
    ((df_state['installation'] == 'Verhalen in Muziek') & (df_state['state_type'] == 'visitorProfileImageTransformation')) |
    ((df_state['installation'] == 'Verhalen in beeld') & (df_state['state_type'] == 'visitorProfileImageTransformation')) |
    ((df_state['installation'] == 'Verleidingstechnieken') & (df_state['state_type'] == 'opinion')) |
    ((df_state['installation'] == 'Verzuiling') & (df_state['state_type'] == 'verzuilingImage')) |
    ((df_state['installation'] == 'Waarzegster') & (df_state['state_type'] == 'visitorProfileImageTransformation')) |
    ((df_state['installation'] == 'Wereldnieuws') & (df_state['state_type'] == 'opinion')) |
    ((df_state['installation'] == 'Wisselvitrine Delen') & (df_state['state_type'] == 'opinion')) |
    ((df_state['installation'] == 'Wisselvitrine Informeren') & (df_state['state_type'] == 'opinion')) |
    ((df_state['installation'] == 'Wisselvitrine Verkopen') & (df_state['state_type'] == 'opinion')) |
    ((df_state['installation'] == 'Wisselvitrine Vertellen') & (df_state['state_type'] == 'opinion'))
]

# Display the filtered dataframe
experiential_engagement_df

#total interaction times for each installations
experiential_engagement_times_per_installation = experiential_engagement_df.groupby('installation').size().reset_index(name='total times')
experiential_engagement_times_per_installation

experiential_engagement_unique_visitors_per_installation = experiential_engagement_df.groupby('installation')['visitor_id'].nunique().reset_index(name='unique_visitor_count')
experiential_engagement_unique_visitors_per_installation

experiential_engagement_calculation = pd.merge(
    experiential_engagement_times_per_installation,
    experiential_engagement_unique_visitors_per_installation,
    on='installation'
)
experiential_engagement_calculation['average_times'] = experiential_engagement_calculation['total times'] / experiential_engagement_calculation['unique_visitor_count']
experiential_engagement_calculation

fig, ax1 = plt.subplots(figsize=(14, 8))

x = range(len(experiential_engagement_calculation))
width = 0.3

ax1.bar([i - width/2 for i in x], experiential_engagement_calculation['total times'], width=width, label='Total Engagement Times', color='skyblue')
ax1.bar([i + width/2 for i in x], experiential_engagement_calculation['unique_visitor_count'], width=width, label='Unique Visitors Number', color='orange')

#axis labels
ax1.set_ylabel('Count')
ax1.set_title('Distribution of Experiential Engagement')
ax1.set_xticks(x)
ax1.set_xticklabels(experiential_engagement_calculation['installation'], rotation=45, ha='right')
ax1.legend(loc='upper center')

# scatter ploot of average times per unique visitor
ax2 = ax1.twinx()
ax2.plot(x, experiential_engagement_calculation['average_times'], 'o-', color='green', label='Average Engagement Times per Visitor')
ax2.set_ylabel('Average Engagement Times per Visitor')
ax2.legend(loc='upper right')

plt.tight_layout()
plt.show()

experiential_engagement_times_per_visitor = experiential_engagement_df.groupby('visitor_id').size().reset_index(name='engagement times')
experiential_engagement_times_per_visitor

experiential_engagement_installation_per_visitor = experiential_engagement_df.groupby('visitor_id')['installation'].nunique().reset_index(name='installation number')
experiential_engagement_installation_per_visitor

experiential_engagement_visitor_pattern = pd.merge(
    experiential_engagement_times_per_visitor,
    experiential_engagement_installation_per_visitor,
    on='visitor_id'
)
experiential_engagement_visitor_pattern

# Create the scatter plot
plt.figure(figsize=(10, 6))
plt.scatter(experiential_engagement_visitor_pattern['engagement times'], experiential_engagement_visitor_pattern['installation number'])

# Add labels and title
plt.xlabel('Number of Experiential Engagement')
plt.ylabel('Number of Unique Installations Engaged With')
plt.title('Experiential Engagement Pattern by Visitor')
plt.grid(True)

# Show the plot
plt.show()

from sklearn.cluster import KMeans
import matplotlib.pyplot as plt

features_experiential = experiential_engagement_visitor_pattern[['engagement times', 'installation number']]
wcss = []
for i in range(1, 11):
    kmeans = KMeans(n_clusters=i, random_state=42)
    kmeans.fit(features_experiential)
    wcss.append(kmeans.inertia_)

plt.plot(range(1, 11), wcss)
plt.title('Elbow Method')
plt.xlabel('Number of clusters')
plt.ylabel('WCSS')
plt.show()

import matplotlib.pyplot as plt
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler

# Select the features for clustering
features_experiential = experiential_engagement_visitor_pattern[['engagement times', 'installation number']]

# Standardize the data
scaler = StandardScaler()
features_scaled = scaler.fit_transform(features_experiential)


kmeans = KMeans(n_clusters=4, random_state=42, n_init=10) # Add n_init
experiential_engagement_visitor_pattern['cluster'] = kmeans.fit_predict(features_experiential)

# Visualize the clusters
plt.figure(figsize=(10, 6))
plt.scatter(experiential_engagement_visitor_pattern['engagement times'], experiential_engagement_visitor_pattern['installation number'], c=experiential_engagement_visitor_pattern['cluster'], cmap=custom_cmap)
plt.xlabel('Number of Experiential Engagement')
plt.ylabel('Number of Unique Installations Engaged With')
plt.title('K-Means Clustering of Visitor Engagement Patterns')
plt.colorbar(label='Cluster')
plt.grid(True)
plt.show()

# Display the number of visitors in each cluster
print("Number of visitors in each cluster:")
print(experiential_engagement_visitor_pattern['cluster'].value_counts())

# Display the characteristics of each cluster (e.g., center values of the features)
cluster_centers = kmeans.cluster_centers_
cluster_features_df = pd.DataFrame(cluster_centers, columns=['engagement times', 'installation number'])
cluster_features_df['cluster'] = cluster_features_df.index
print("\nCluster Centers:")
print(cluster_features_df)

# prompt: 我想用DBSCAN来去除噪点异常值

import matplotlib.pyplot as plt
from sklearn.cluster import DBSCAN

# Select the features for clustering
features_experiential = experiential_engagement_visitor_pattern[['engagement times', 'installation number']]

# Standardize the data (important for distance-based algorithms like DBSCAN)
scaler = StandardScaler()
features_scaled = scaler.fit_transform(features_experiential)

# Initialize DBSCAN
# eps: The maximum distance between two samples for one to be considered as in the neighborhood of the other.
# min_samples: The number of samples (or total weight) in a neighborhood for a point to be considered as a core point. This includes the point itself.
# You will likely need to tune these parameters based on your data
dbscan = DBSCAN(eps=0.5, min_samples=5)

# Fit the model and predict labels
# -1 indicates noise points (outliers)
labels = dbscan.fit_predict(features_scaled)

# Add the DBSCAN labels to the DataFrame
experiential_engagement_visitor_pattern['dbscan_label'] = labels

# Filter out the noise points (outliers)
experiential_engagement_visitor_pattern_cleaned_dbscan = experiential_engagement_visitor_pattern[
    experiential_engagement_visitor_pattern['dbscan_label'] != -1
]
# Display the number of outliers removed by DBSCAN
outlier_count_dbscan_experiential = (labels == -1).sum()
total_count_dbscan_experiential = len(labels)
outlier_percentage_dbscan_experiential = (outlier_count_dbscan_experiential / total_count_dbscan_experiential) * 100 if total_count_dbscan_experiential > 0 else 0

print(f"Using DBSCAN to remove Outliers number: {outlier_count_dbscan}")
print(f"Removed Outliers Percentage: {outlier_percentage_dbscan:.2f}%")

# Visualize the clusters found by DBSCAN (excluding outliers for better visualization)
plt.figure(figsize=(10, 6))
# Plot non-outliers with their cluster labels
plt.scatter(experiential_engagement_visitor_pattern_cleaned_dbscan['engagement times'],
            experiential_engagement_visitor_pattern_cleaned_dbscan['installation number'],
            cmap='skyblue',
            label='Non-Outliers')

# Optionally, plot outliers in a different color/marker
outliers_dbscan_experiential = experiential_engagement_visitor_pattern[experiential_engagement_visitor_pattern['dbscan_label'] == -1]
plt.scatter(outliers_dbscan_experiential['engagement times'],
            outliers_dbscan_experiential['installation number'],
            c='red',
            marker='x',
            label='Outliers (-1)')

plt.xlabel('Number of Experiential Engagement')
plt.ylabel('Number of Unique Installations Engaged With')
plt.title('DBSCAN Clustering of Visitor Engagement Patterns (Outliers in Red)')
plt.legend()
plt.grid(True)
plt.show()

# Display the number of visitors in each cluster found by DBSCAN
print("\nNumber of visitors in each cluster (DBSCAN):")
print(responsive_interaction_visitor_pattern['dbscan_label'].value_counts())

from sklearn.cluster import KMeans
import matplotlib.pyplot as plt

features_experiential_cleaned = experiential_engagement_visitor_pattern_cleaned_dbscan[['engagement times', 'installation number']]
wcss = []
for i in range(1, 11):
    kmeans = KMeans(n_clusters=i, random_state=42)
    kmeans.fit(features_experiential_cleaned)
    wcss.append(kmeans.inertia_)

plt.plot(range(1, 11), wcss)
plt.title('Elbow Method (cleaned)')
plt.xlabel('Number of clusters')
plt.ylabel('WCSS')
plt.show()

import matplotlib.pyplot as plt
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler

# Select the features for clustering
features_experiential_cleaned = experiential_engagement_visitor_pattern_cleaned_dbscan[['engagement times', 'installation number']]

# Standardize the data
scaler = StandardScaler()
features_scaled = scaler.fit_transform(features_experiential_cleaned)


kmeans = KMeans(n_clusters=3, random_state=42, n_init=10) # Add n_init
experiential_engagement_visitor_pattern_cleaned_dbscan['cluster'] = kmeans.fit_predict(features_experiential_cleaned)

# Visualize the clusters
plt.figure(figsize=(10, 6))
plt.scatter(experiential_engagement_visitor_pattern_cleaned_dbscan['engagement times'], experiential_engagement_visitor_pattern_cleaned_dbscan['installation number'], c=experiential_engagement_visitor_pattern_cleaned_dbscan['cluster'], cmap=custom_cmap)
plt.xlabel('Number of Experiential Engagement')
plt.ylabel('Number of Unique Installations Engaged With')
plt.title('K-Means Clustering of Visitor Engagement Patterns (Cleaned)')
plt.colorbar(label='Cluster')
plt.grid(True)
plt.show()

# Display the number of visitors in each cluster
print("Number of visitors in each cluster:")
print(experiential_engagement_visitor_pattern_cleaned_dbscan['cluster'].value_counts())

# Display the characteristics of each cluster (e.g., center values of the features)
cluster_centers = kmeans.cluster_centers_
cluster_features_df = pd.DataFrame(cluster_centers, columns=['engagement times', 'installation number'])
cluster_features_df['cluster'] = cluster_features_df.index
print("\nCluster Centers:")
print(cluster_features_df)

engagement_installation_combination_counts = experiential_engagement_visitor_pattern_cleaned_dbscan.groupby(['engagement times', 'installation number', 'cluster']).size().reset_index(name='unique visitor count')
engagement_installation_combination_counts

plt.figure(figsize=(10, 6)) # Increase figure size for better visibility if needed

# Use the aggregated data for plotting
plt.scatter(
    engagement_installation_combination_counts['engagement times'],
    engagement_installation_combination_counts['installation number'],
    s=engagement_installation_combination_counts['unique visitor count'] * 5, # 's' is the size parameter. Multiply by a factor to make sizes visible.
    alpha=0.7, # Add some transparency
    edgecolors='w', # White edge color for better visibility
    linewidth=0.5,
    c=engagement_installation_combination_counts['cluster'],
    cmap=custom_cmap
)

# Add labels and title
plt.xlabel('Number of Experiential Engagement')
plt.ylabel('Number of Unique Installations Engaged With')
plt.title('Experiential Engagement Pattern by Visitor (Size by Count)')
plt.grid(True)

plt.colorbar(label='Cluster')
# Optional: Add a legend for size
# This is a bit trickier with scatter plots and custom sizes.
# A common way is to plot dummy points for representative sizes.
for count in [1, 5, 10, 20, 50]: # Example counts to show in legend
     plt.scatter([], [], s=count * 5, label=f'{count} visitors', color='c', alpha=0.6)
plt.legend(scatterpoints=1, frameon=False, labelspacing=1, title='Visitor Count')


# Show the plot
plt.show()

"""# Correlation

"""

visitor_clusters = pd.merge(
    responsive_interaction_visitor_pattern_cleaned_dbscan[['visitor_id', 'cluster']],
    experiential_engagement_visitor_pattern_cleaned_dbscan[['visitor_id', 'cluster']],
    on='visitor_id',
    how='outer',
    suffixes=('_responsive', '_experiential') # Add suffixes to differentiate cluster columns
)

# Rename the cluster columns for clarity
visitor_clusters.rename(columns={
    'cluster_responsive': 'responsive_cluster',
    'cluster_experiential': 'experiential_cluster'
}, inplace=True)

# Display the resulting dataframe
print("Visitor Clustering Results:")
print(visitor_clusters)

# Create a crosstab of the two cluster assignments
crosstab_clusters = pd.crosstab(
    visitor_clusters['responsive_cluster'],
    visitor_clusters['experiential_cluster'],
    margins=True # Add row and column sums
)

# Display the crosstab
print("\nCrosstab of Responsive Interaction Clusters and Experiential Engagement Clusters:")
print(crosstab_clusters)

import matplotlib.pyplot as plt
import seaborn as sns

# Define the mapping from cluster numbers to desired labels
cluster_label_map = {
    0.0: 'Moderate',
    1.0: 'Low',
    2.0: 'High'
}

# Rename the index (Responsive Interaction clusters) using the map
# Ensure the index contains the keys (0.0, 1.0, 2.0) before renaming
# We use .astype(float) just in case the index is not already float type
crosstab_clusters_renamed = crosstab_clusters.rename(index=cluster_label_map)

# Rename the columns (Experiential Engagement clusters) using the map
# Ensure the columns contain the keys (0.0, 1.0, 2.0) before renaming
# We use .astype(float) just in case the columns are not already float type
crosstab_clusters_renamed = crosstab_clusters_renamed.rename(columns=cluster_label_map)

plt.figure(figsize=(6.25, 5))
sns.heatmap(crosstab_clusters_renamed, annot=True, fmt='d', cmap='YlGnBu')
plt.title('Heatmap of Visitor Clusters (Responsive vs. Experiential)')
plt.xlabel('Experiential Engagement Cluster')
plt.ylabel('Responsive Interaction Cluster')
plt.show()

import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
from scipy.stats import chi2_contingency

# Perform the chi-squared test on the crosstab (excluding margins)
chi2, p, dof, expected = chi2_contingency(crosstab_clusters_renamed.iloc[:-1, :-1])

print(f"\nChi-Squared Test Results:")
print(f"Chi-squared statistic: {chi2:.2f}")
print(f"P-value: {p:.4f}")
print(f"Degrees of freedom: {dof}")
print("Expected frequencies (for relationship analysis):")
print(pd.DataFrame(expected, index=crosstab_clusters_renamed.index[:-1], columns=crosstab_clusters_renamed.columns[:-1]))

# Calculate the standardized residuals
# (Observed - Expected) / sqrt(Expected)
standardized_residuals = (crosstab_clusters_renamed.iloc[:-1, :-1] - expected) / np.sqrt(expected)

print("\nStandardized Residuals:")
print(standardized_residuals)

# Visualize the standardized residuals as a heatmap
plt.figure(figsize=(6.25, 5))
sns.heatmap(standardized_residuals, annot=True, fmt='.2f', cmap='YlGnBu', center=0)
plt.title('Standardized Residuals Heatmap (Responsive vs. Experiential)')
plt.xlabel('Experiential Engagement Cluster')
plt.ylabel('Responsive Interaction Cluster')
plt.show()

from scipy.stats import chi2_contingency
import numpy as np

# Make sure to use the crosstab_clusters dataframe, excluding the margins row and column if they exist
# We drop the 'All' row and column if margins were included
contingency_table = crosstab_clusters.iloc[:-1, :-1] if 'All' in crosstab_clusters.index and 'All' in crosstab_clusters.columns else crosstab_clusters

# Calculate the Chi-squared statistic, p-value, degrees of freedom, and expected frequencies
chi2, p, dof, expected = chi2_contingency(contingency_table)

# Calculate the total number of observations
n = contingency_table.sum().sum()

# Calculate min(rows-1, columns-1)
min_dim = min(contingency_table.shape) - 1

# Calculate Cramér's V
# Handle the case where min_dim is 0 to avoid division by zero
cramers_v = np.sqrt((chi2 / n) / min_dim) if min_dim > 0 else 0

print(f"\nChi-squared statistic: {chi2}")
print(f"P-value: {p}")
print(f"Degrees of freedom: {dof}")
print(f"Cramér's V: {cramers_v}")

# Interpret Cramér's V
if cramers_v < 0.1:
    print("Cramér's V indicates negligible association.")
elif cramers_v < 0.3:
    print("Cramér's V indicates weak association.")
elif cramers_v < 0.5:
    print("Cramér's V indicates moderate association.")
else:
    print("Cramér's V indicates strong association.")